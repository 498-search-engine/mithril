# Crawler config
workers=4
concurrent_requests=100
data_directory=data
metrics_port=9000

# Time between crawls on a single host
default_crawl_delay_ms=2000

# Middle queue internal configuration
middle_queue.queue_count=400
middle_queue.url_batch_size=10
middle_queue.host_url_limit=25
middle_queue.utilization_target=0.5

# robots.txt configuration
concurrent_robots_requests=200

# Take snapshot of state every 30 minutes
snapshot_period_seconds=1800

# Seed urls, specified in seed_list.conf
seed_url_file=seed_list.conf

# Blacklisted hosts, specified in blacklisted_hosts.conf
blacklist_host_file=blacklisted_hosts.conf
